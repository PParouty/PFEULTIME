"""
Orchestrateur LangGraph - Coordonne les agents et les outils pour r√©pondre aux requ√™tes.
C'est le coeur du syst√®me multi-agents.
"""

import json
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import openai  # Pour capturer BadRequestError

from agents.planner import PlannerAgent
from agents.prioritizer import PrioritizerAgent
from agents.summarizer import SummarizerAgent
from tools.elasticsearch_tools import TOOLS, get_es_tools


class GraphState(TypedDict):
    """√âtat du graphe partag√© entre tous les noeuds."""
    # Requ√™te initiale
    query: str
    # Plan d'action g√©n√©r√©
    plan: str
    # R√©sultats d'exploration accumul√©s
    exploration_results: list[dict]
    # Noeuds explor√©s (pour √©viter les boucles)
    explored_nodes: list[dict]
    # Noeuds candidats √† explorer
    candidate_nodes: list[dict]
    # Compteur d'it√©rations (pour √©viter boucles infinies)
    iteration: int
    # R√©sum√© final
    summary: str
    # Messages pour le LLM ex√©cuteur
    messages: Annotated[list, add_messages]
    # Flag de fin
    finished: bool
    # Flag si le contexte a √©t√© d√©pass√© (r√©sultats partiels)
    context_exceeded: bool


class GraphOrchestrator:
    """
    Orchestrateur principal utilisant LangGraph.
    Coordonne le Planificateur, l'Ex√©cuteur (avec outils), le Priorisation et le Summarizer.
    """

    MAX_ITERATIONS = 10  # Limite de s√©curit√©

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name

        # Initialiser les agents
        self.planner = PlannerAgent(model_name=model_name)
        self.prioritizer = PrioritizerAgent(model_name=model_name)
        self.summarizer = SummarizerAgent(model_name=model_name)

        # LLM ex√©cuteur avec outils
        self.executor_llm = ChatOpenAI(model=model_name, temperature=0.0)
        self.executor_llm_with_tools = self.executor_llm.bind_tools(TOOLS)

        # Construire le graphe
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """Construit le graphe LangGraph."""

        # Cr√©er le graphe
        workflow = StateGraph(GraphState)

        # Ajouter les noeuds
        workflow.add_node("planner", self._plan_node)
        workflow.add_node("executor", self._executor_node)
        workflow.add_node("tools", ToolNode(TOOLS))
        workflow.add_node("prioritizer", self._prioritizer_node)
        workflow.add_node("summarizer", self._summarizer_node)

        # D√©finir le point d'entr√©e
        workflow.set_entry_point("planner")

        # Ajouter les transitions
        workflow.add_edge("planner", "executor")

        # Apr√®s executor: soit appeler tools, soit passer au prioritizer/summarizer
        workflow.add_conditional_edges(
            "executor",
            self._should_use_tools,
            {
                "tools": "tools",
                "prioritizer": "prioritizer",
                "summarizer": "summarizer"
            }
        )

        # Apr√®s tools: retourner √† executor
        workflow.add_edge("tools", "executor")

        # Apr√®s prioritizer: retourner √† executor ou terminer
        workflow.add_conditional_edges(
            "prioritizer",
            self._should_continue_exploration,
            {
                "executor": "executor",
                "summarizer": "summarizer"
            }
        )

        # Summarizer est la fin
        workflow.add_edge("summarizer", END)

        return workflow.compile()

    def _plan_node(self, state: GraphState) -> dict:
        """Noeud Planificateur - Cr√©e le plan d'action."""
        print("\nüìã [Planificateur] Cr√©ation du plan d'action...")

        plan = self.planner.create_plan(state["query"])
        print(f"Plan cr√©√©:\n{plan}\n")

        # Pr√©parer le message syst√®me pour l'ex√©cuteur
        system_message = SystemMessage(content=f"""Tu es un agent ex√©cuteur qui suit un plan d'action pour explorer un graphe d'entreprises.

PLAN √Ä SUIVRE:
{plan}

INSTRUCTIONS:
1. Ex√©cute le plan √©tape par √©tape en utilisant les outils disponibles
2. Apr√®s chaque outil, analyse le r√©sultat
3. Adapte le plan si n√©cessaire (ex: si une entit√© n'est pas trouv√©e)
4. Quand le plan est compl√©t√©, dis "EXPLORATION_COMPLETE"

Commence par la premi√®re √©tape du plan.""")

        return {
            "plan": plan,
            "messages": [system_message],
            "iteration": 0,
            "exploration_results": [],
            "explored_nodes": [],
            "candidate_nodes": [],
            "finished": False
        }

    def _executor_node(self, state: GraphState) -> dict:
        """Noeud Ex√©cuteur - Ex√©cute le plan en utilisant les outils."""
        print(f"\nüîß [Ex√©cuteur] It√©ration {state['iteration'] + 1}...")

        try:
            # Appeler le LLM avec les outils
            response = self.executor_llm_with_tools.invoke(state["messages"])

            return {
                "messages": [response],
                "iteration": state["iteration"] + 1
            }

        except openai.BadRequestError as e:
            # G√©rer le d√©passement de contexte
            if "context_length_exceeded" in str(e):
                print("\n‚ö†Ô∏è  [Ex√©cuteur] Contexte trop grand - passage au r√©sum√© avec les r√©sultats partiels...")

                # Cr√©er un message qui forcera le passage au summarizer
                error_message = AIMessage(content="""EXPLORATION_COMPLETE

Note: L'exploration a √©t√© interrompue car le contexte √©tait trop volumineux.
Les r√©sultats ci-dessous sont partiels mais contiennent les informations trouv√©es jusqu'√† pr√©sent.""")

                return {
                    "messages": [error_message],
                    "iteration": state["iteration"] + 1,
                    "context_exceeded": True
                }
            else:
                # Autre erreur OpenAI - la propager
                raise

    def _should_use_tools(self, state: GraphState) -> str:
        """D√©cide si on doit appeler des outils ou passer √† la suite."""
        last_message = state["messages"][-1]

        # V√©rifier si le LLM veut utiliser des outils
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            print("   ‚Üí Appel d'outils...")
            return "tools"

        # V√©rifier si l'exploration est termin√©e
        if "EXPLORATION_COMPLETE" in str(last_message.content):
            print("   ‚Üí Exploration termin√©e, passage au r√©sum√©...")
            return "summarizer"

        # V√©rifier la limite d'it√©rations
        if state["iteration"] >= self.MAX_ITERATIONS:
            print("   ‚Üí Limite d'it√©rations atteinte, passage au r√©sum√©...")
            return "summarizer"

        # Sinon, passer au prioritizer pour d√©cider de la suite
        print("   ‚Üí Passage au priorisation...")
        return "prioritizer"

    def _prioritizer_node(self, state: GraphState) -> dict:
        """Noeud Priorisation - D√©cide des prochains noeuds √† explorer."""
        print("\nüéØ [Priorisation] Analyse des prochaines √©tapes...")

        # Extraire les r√©sultats actuels des messages
        results_summary = self._extract_results_summary(state["messages"])

        priorities = self.prioritizer.prioritize(
            objective=state["query"],
            explored_nodes=state["explored_nodes"],
            candidate_nodes=state["candidate_nodes"],
            partial_results=results_summary
        )

        print(f"Priorit√©s:\n{priorities}\n")

        # Ajouter les priorit√©s comme message pour l'ex√©cuteur
        priority_message = HumanMessage(content=f"""Voici les noeuds prioritaires √† explorer:

{priorities}

Continue l'exploration ou dis "EXPLORATION_COMPLETE" si tu as assez d'informations.""")

        return {"messages": [priority_message]}

    def _should_continue_exploration(self, state: GraphState) -> str:
        """D√©cide si on continue l'exploration ou si on r√©sume."""
        # Simple heuristique: si on a d√©pass√© un certain nombre d'it√©rations
        if state["iteration"] >= self.MAX_ITERATIONS // 2:
            return "summarizer"
        return "executor"

    def _summarizer_node(self, state: GraphState) -> dict:
        """Noeud Summarizer - Produit le r√©sum√© final."""
        print("\nüìù [Summarizer] Production du r√©sum√© final...")

        # Extraire tous les r√©sultats des messages
        raw_results = self._extract_results_summary(state["messages"])

        # Ajouter une note si le contexte a √©t√© d√©pass√©
        if state.get("context_exceeded", False):
            raw_results += "\n\n‚ö†Ô∏è NOTE: L'exploration a √©t√© interrompue (contexte trop volumineux). Ces r√©sultats sont partiels."

        summary = self.summarizer.summarize(
            original_query=state["query"],
            plan=state["plan"],
            raw_results=raw_results
        )

        # Ajouter un avertissement en d√©but de r√©sum√© si n√©cessaire
        if state.get("context_exceeded", False):
            summary = "‚ö†Ô∏è **R√©sultats partiels** (exploration interrompue)\n\n" + summary

        return {"summary": summary, "finished": True}

    def _extract_results_summary(self, messages: list) -> str:
        """Extrait un r√©sum√© des r√©sultats √† partir des messages."""
        results = []
        for msg in messages:
            if hasattr(msg, "content") and msg.content:
                # Filtrer les messages syst√®me
                if isinstance(msg, (AIMessage, HumanMessage)):
                    content = str(msg.content)
                    if len(content) > 50:  # Ignorer les messages tr√®s courts
                        results.append(content[:500])  # Limiter la taille

        return "\n---\n".join(results[-10:])  # Garder les 10 derniers

    def run(self, query: str) -> str:
        """
        Ex√©cute une requ√™te d'investigation.

        Args:
            query: La question de l'utilisateur en langage naturel

        Returns:
            Le r√©sum√© des r√©sultats
        """
        print(f"\n{'='*60}")
        print(f"üîç Nouvelle investigation: {query}")
        print(f"{'='*60}")

        # √âtat initial
        initial_state = {
            "query": query,
            "plan": "",
            "exploration_results": [],
            "explored_nodes": [],
            "candidate_nodes": [],
            "iteration": 0,
            "summary": "",
            "messages": [],
            "finished": False,
            "context_exceeded": False
        }

        # Ex√©cuter le graphe
        final_state = self.graph.invoke(initial_state)

        return final_state.get("summary", "Aucun r√©sultat trouv√©.")
