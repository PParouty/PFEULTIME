"""
Orchestrateur LangGraph - Coordonne les agents et les outils pour r√©pondre aux requ√™tes.
C'est le coeur du syst√®me multi-agents.
"""

import json
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
import openai  # Pour capturer BadRequestError

from agents.planner import PlannerAgent
from agents.prioritizer import PrioritizerAgent
from agents.summarizer import SummarizerAgent
from tools.elasticsearch_tools import TOOLS, get_es_tools


class GraphState(TypedDict):
    """√âtat du graphe partag√© entre tous les noeuds."""
    # Requ√™te initiale
    query: str
    # Plan d'action g√©n√©r√©
    plan: str
    # R√©sultats d'exploration accumul√©s
    exploration_results: list[dict]
    # Noeuds explor√©s (pour √©viter les boucles)
    explored_nodes: list[dict]
    # Noeuds candidats √† explorer
    candidate_nodes: list[dict]
    # Compteur d'it√©rations (pour √©viter boucles infinies)
    iteration: int
    # R√©sum√© final
    summary: str
    # Messages pour le LLM ex√©cuteur
    messages: Annotated[list, add_messages]
    # Flag de fin
    finished: bool
    # Flag si le contexte a √©t√© d√©pass√© (r√©sultats partiels)
    context_exceeded: bool


class GraphOrchestrator:
    """
    Orchestrateur principal utilisant LangGraph.
    Coordonne le Planificateur, l'Ex√©cuteur (avec outils), le Priorisation et le Summarizer.
    """

    MAX_ITERATIONS = 15  # Limite de s√©curit√© (augment√©e pour plus d'exploration)
    CONDENSE_THRESHOLD = 25  # Seuil pour d√©clencher la condensation du contexte
    KEEP_RECENT_MESSAGES = 6  # Messages r√©cents √† garder apr√®s condensation

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name

        # Initialiser les agents
        self.planner = PlannerAgent(model_name=model_name)
        self.prioritizer = PrioritizerAgent(model_name=model_name)
        self.summarizer = SummarizerAgent(model_name=model_name)

        # LLM ex√©cuteur avec outils
        self.executor_llm = ChatOpenAI(model=model_name, temperature=0.0)
        self.executor_llm_with_tools = self.executor_llm.bind_tools(TOOLS)

        # Construire le graphe
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """Construit le graphe LangGraph."""

        # Cr√©er le graphe
        workflow = StateGraph(GraphState)

        # Ajouter les noeuds
        workflow.add_node("planner", self._plan_node)
        workflow.add_node("executor", self._executor_node)
        workflow.add_node("tools", ToolNode(TOOLS))
        workflow.add_node("prioritizer", self._prioritizer_node)
        workflow.add_node("summarizer", self._summarizer_node)

        # D√©finir le point d'entr√©e
        workflow.set_entry_point("planner")

        # Ajouter les transitions
        workflow.add_edge("planner", "executor")

        # Apr√®s executor: soit appeler tools, soit passer au prioritizer/summarizer
        workflow.add_conditional_edges(
            "executor",
            self._should_use_tools,
            {
                "tools": "tools",
                "prioritizer": "prioritizer",
                "summarizer": "summarizer"
            }
        )

        # Apr√®s tools: retourner √† executor
        workflow.add_edge("tools", "executor")

        # Apr√®s prioritizer: retourner √† executor ou terminer
        workflow.add_conditional_edges(
            "prioritizer",
            self._should_continue_exploration,
            {
                "executor": "executor",
                "summarizer": "summarizer"
            }
        )

        # Summarizer est la fin
        workflow.add_edge("summarizer", END)

        return workflow.compile()

    def _plan_node(self, state: GraphState) -> dict:
        """Noeud Planificateur - Cr√©e le plan d'action."""
        print("\n[Planificateur] Cr√©ation du plan d'action...")

        plan = self.planner.create_plan(state["query"])
        print(f"Plan cr√©√©:\n{plan}\n")

        # Pr√©parer le message syst√®me pour l'ex√©cuteur
        system_message = SystemMessage(content=f"""Tu es un agent ex√©cuteur qui suit un plan d'action pour explorer un graphe d'entreprises.

PLAN √Ä SUIVRE:
{plan}

INSTRUCTIONS:
1. Ex√©cute le plan √©tape par √©tape en utilisant les outils disponibles
2. Apr√®s chaque outil, analyse le r√©sultat
3. Adapte le plan si n√©cessaire (ex: si une entit√© n'est pas trouv√©e)
4. Quand le plan est compl√©t√©, dis "EXPLORATION_COMPLETE"

Commence par la premi√®re √©tape du plan.""")

        return {
            "plan": plan,
            "messages": [system_message],
            "iteration": 0,
            "exploration_results": [],
            "explored_nodes": [],
            "candidate_nodes": [],
            "finished": False
        }

    def _condense_context(self, messages: list, query: str) -> list:
        """
        Condense le contexte en r√©sumant les d√©couvertes interm√©diaires.

        Au lieu de tronquer (perdre l'info), on R√âSUME (pr√©server l'info condens√©e).
        C'est la vraie Option 2 : optimisation intelligente du contexte.
        """
        print("   üóúÔ∏è  Condensation du contexte en cours...")

        # S√©parer le message syst√®me
        system_msg = None
        other_messages = messages
        if messages and isinstance(messages[0], SystemMessage):
            system_msg = messages[0]
            other_messages = messages[1:]

        # Identifier les "unit√©s" compl√®tes (pr√©server paires tool_calls/tool)
        units = []
        i = len(other_messages) - 1
        while i >= 0:
            msg = other_messages[i]
            if isinstance(msg, ToolMessage):
                unit = [msg]
                i -= 1
                while i >= 0 and isinstance(other_messages[i], ToolMessage):
                    unit.insert(0, other_messages[i])
                    i -= 1
                if i >= 0 and isinstance(other_messages[i], AIMessage):
                    ai_msg = other_messages[i]
                    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
                        unit.insert(0, ai_msg)
                        i -= 1
                units.insert(0, unit)
            else:
                units.insert(0, [msg])
                i -= 1

        # Garder les 2-3 derni√®res unit√©s compl√®tes pour continuit√©
        recent_units = units[-3:] if len(units) > 3 else units
        recent_messages = [msg for unit in recent_units for msg in unit]

        # Messages √† r√©sumer = tout sauf les r√©cents
        units_to_summarize = units[:-3] if len(units) > 3 else []
        messages_to_summarize = [msg for unit in units_to_summarize for msg in unit]

        if not messages_to_summarize:
            return messages  # Rien √† condenser

        # Extraire le contenu √† r√©sumer
        content_to_summarize = []
        for msg in messages_to_summarize:
            if isinstance(msg, AIMessage) and msg.content:
                content_to_summarize.append(f"[Assistant]: {msg.content[:500]}")
            elif isinstance(msg, ToolMessage):
                # R√©sumer les r√©sultats d'outils (souvent volumineux)
                content = str(msg.content)[:300]
                content_to_summarize.append(f"[Outil]: {content}")
            elif isinstance(msg, HumanMessage):
                content_to_summarize.append(f"[Contexte]: {msg.content[:200]}")

        # Demander au LLM de r√©sumer
        summary_prompt = f"""R√©sume de mani√®re CONCISE les d√©couvertes faites jusqu'ici pour r√©pondre √†: "{query}"

Informations √† r√©sumer:
{chr(10).join(content_to_summarize[-15:])}

Instructions:
- Liste les entit√©s trouv√©es (companies, investors) avec leurs IDs
- Liste les faits cl√©s d√©couverts (montants, dates, relations)
- Sois TR√àS concis (max 300 mots)
- Format: bullet points"""

        try:
            summary_response = self.executor_llm.invoke([HumanMessage(content=summary_prompt)])
            summary_content = summary_response.content
        except Exception as e:
            # En cas d'erreur, faire un r√©sum√© basique
            summary_content = "R√©sum√© non disponible - exploration en cours."

        # Cr√©er le message de r√©sum√© condens√©
        condensed_message = HumanMessage(content=f"""R√âSUM√â DES D√âCOUVERTES PR√âC√âDENTES:
{summary_content}

Continue l'exploration √† partir de ces informations.""")

        # Reconstruire la liste de messages
        result = []
        if system_msg:
            result.append(system_msg)
        result.append(condensed_message)
        result.extend(recent_messages)

        print(f"   Contexte condens√©: {len(messages)} ‚Üí {len(result)} messages")
        return result

    def _truncate_messages_safely(self, messages: list, max_messages: int) -> list:
        """
        Tronque les messages en pr√©servant les paires tool_calls/tool_response.

        OpenAI exige que les ToolMessage suivent imm√©diatement le AIMessage
        avec tool_calls correspondant. Cette m√©thode garantit qu'on ne coupe
        jamais au milieu d'une telle paire.
        """
        if len(messages) <= max_messages:
            return messages

        # S√©parer le message syst√®me (√† toujours garder)
        system_msg = None
        other_messages = messages
        if messages and isinstance(messages[0], SystemMessage):
            system_msg = messages[0]
            other_messages = messages[1:]
            max_messages -= 1  # R√©server une place pour le syst√®me

        # Identifier les "unit√©s" de messages (groupes tool_calls + r√©ponses)
        # On parcourt de la fin vers le d√©but pour garder les plus r√©cents
        units = []
        i = len(other_messages) - 1

        while i >= 0:
            msg = other_messages[i]

            if isinstance(msg, ToolMessage):
                # C'est une r√©ponse d'outil - trouver le AIMessage avec tool_calls
                unit = [msg]
                i -= 1

                # Collecter toutes les ToolMessage cons√©cutives
                while i >= 0 and isinstance(other_messages[i], ToolMessage):
                    unit.insert(0, other_messages[i])
                    i -= 1

                # Le message pr√©c√©dent doit √™tre le AIMessage avec tool_calls
                if i >= 0 and isinstance(other_messages[i], AIMessage):
                    ai_msg = other_messages[i]
                    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
                        unit.insert(0, ai_msg)
                        i -= 1

                units.insert(0, unit)
            else:
                # Message standalone (HumanMessage, AIMessage sans tool_calls)
                units.insert(0, [msg])
                i -= 1

        # S√©lectionner les unit√©s les plus r√©centes qui tiennent dans la limite
        selected_messages = []
        for unit in reversed(units):
            if len(selected_messages) + len(unit) <= max_messages:
                selected_messages = unit + selected_messages
            else:
                break

        # Reconstruire la liste avec le message syst√®me
        if system_msg:
            return [system_msg] + selected_messages
        return selected_messages

    def _executor_node(self, state: GraphState) -> dict:
        """Noeud Ex√©cuteur - Ex√©cute le plan en utilisant les outils."""
        print(f"\n[Ex√©cuteur] It√©ration {state['iteration'] + 1}...")

        # OPTION 2 (vraie): Condensation intelligente du contexte
        # Au lieu de tronquer (perdre info), on R√âSUME (pr√©server info condens√©e)
        messages = state["messages"]
        if len(messages) > self.CONDENSE_THRESHOLD:
            messages = self._condense_context(messages, state["query"])

        try:
            # Appeler le LLM avec les outils (messages optimis√©s)
            response = self.executor_llm_with_tools.invoke(messages)

            return {
                "messages": [response],
                "iteration": state["iteration"] + 1
            }

        except openai.BadRequestError as e:
            # G√©rer le d√©passement de contexte
            if "context_length_exceeded" in str(e):
                print("\n[Ex√©cuteur] Contexte trop grand - passage au r√©sum√© avec les r√©sultats partiels...")

                # Cr√©er un message qui forcera le passage au summarizer
                error_message = AIMessage(content="""EXPLORATION_COMPLETE

Note: L'exploration a √©t√© interrompue car le contexte √©tait trop volumineux.
Les r√©sultats ci-dessous sont partiels mais contiennent les informations trouv√©es jusqu'√† pr√©sent.""")

                return {
                    "messages": [error_message],
                    "iteration": state["iteration"] + 1,
                    "context_exceeded": True
                }
            else:
                # Autre erreur OpenAI - la propager
                raise

    def _should_use_tools(self, state: GraphState) -> str:
        """D√©cide si on doit appeler des outils ou passer √† la suite."""
        last_message = state["messages"][-1]

        # V√©rifier si le LLM veut utiliser des outils
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            print("   ‚Üí Appel d'outils...")
            return "tools"

        # V√©rifier si l'exploration est termin√©e
        if "EXPLORATION_COMPLETE" in str(last_message.content):
            print("   ‚Üí Exploration termin√©e, passage au r√©sum√©...")
            return "summarizer"

        # V√©rifier la limite d'it√©rations
        if state["iteration"] >= self.MAX_ITERATIONS:
            print("   ‚Üí Limite d'it√©rations atteinte, passage au r√©sum√©...")
            return "summarizer"

        # Sinon, passer au prioritizer pour d√©cider de la suite
        print("   ‚Üí Passage au priorisation...")
        return "prioritizer"

    def _prioritizer_node(self, state: GraphState) -> dict:
        """Noeud Priorisation - D√©cide des prochains noeuds √† explorer."""
        print("\n[Priorisation] Analyse des prochaines √©tapes...")

        # Extraire les r√©sultats actuels des messages
        results_summary = self._extract_results_summary(state["messages"])

        priorities = self.prioritizer.prioritize(
            objective=state["query"],
            explored_nodes=state["explored_nodes"],
            candidate_nodes=state["candidate_nodes"],
            partial_results=results_summary
        )

        print(f"Priorit√©s:\n{priorities}\n")

        # Ajouter les priorit√©s comme message pour l'ex√©cuteur
        priority_message = HumanMessage(content=f"""Voici les noeuds prioritaires √† explorer:

{priorities}

Continue l'exploration ou dis "EXPLORATION_COMPLETE" si tu as assez d'informations.""")

        return {"messages": [priority_message]}

    def _should_continue_exploration(self, state: GraphState) -> str:
        """D√©cide si on continue l'exploration ou si on r√©sume."""
        # Simple heuristique: si on a d√©pass√© un certain nombre d'it√©rations
        if state["iteration"] >= self.MAX_ITERATIONS // 2:
            return "summarizer"
        return "executor"

    def _summarizer_node(self, state: GraphState) -> dict:
        """Noeud Summarizer - Produit le r√©sum√© final."""
        print("\n[Summarizer] Production du r√©sum√© final...")

        # Extraire tous les r√©sultats des messages
        raw_results = self._extract_results_summary(state["messages"])

        # Ajouter une note si le contexte a √©t√© d√©pass√©
        if state.get("context_exceeded", False):
            raw_results += "\n\nNOTE: L'exploration a √©t√© interrompue (contexte trop volumineux). Ces r√©sultats sont partiels."

        summary = self.summarizer.summarize(
            original_query=state["query"],
            plan=state["plan"],
            raw_results=raw_results
        )

        # Ajouter un avertissement en d√©but de r√©sum√© si n√©cessaire
        if state.get("context_exceeded", False):
            summary = "**R√©sultats partiels** (exploration interrompue)\n\n" + summary

        return {"summary": summary, "finished": True}

    def _extract_results_summary(self, messages: list) -> str:
        """Extrait un r√©sum√© des r√©sultats √† partir des messages."""
        import json
        results = []

        for msg in messages:
            if hasattr(msg, "content") and msg.content:
                content = str(msg.content)

                # Inclure les ToolMessages (contiennent les vraies donn√©es!)
                if isinstance(msg, ToolMessage):
                    # Essayer de parser le JSON pour extraire intelligemment
                    try:
                        data = json.loads(content) if content.startswith('{') or content.startswith('[') else None
                        if data:
                            summary = self._summarize_tool_data(data)
                            results.append(f"[Outil]: {summary}")
                        else:
                            results.append(f"[Outil]: {content[:1500]}")
                    except json.JSONDecodeError:
                        results.append(f"[Outil]: {content[:1500]}")

                # Messages de l'assistant (analyses, conclusions)
                elif isinstance(msg, AIMessage):
                    if len(content) > 50:
                        results.append(f"[Assistant]: {content[:1000]}")

                # Messages humains/contexte
                elif isinstance(msg, HumanMessage):
                    if len(content) > 50 and "R√âSUM√â DES D√âCOUVERTES" not in content:
                        results.append(f"[Contexte]: {content[:500]}")

        # Garder plus de messages pour avoir un meilleur contexte
        return "\n---\n".join(results[-20:])

    def _summarize_tool_data(self, data: dict | list) -> str:
        """R√©sume intelligemment les donn√©es d'un outil."""
        if isinstance(data, list):
            # Liste d'entit√©s - extraire les labels
            labels = [item.get('label', item.get('id', '?')) for item in data[:50]]
            if len(data) > 50:
                return f"{len(data)} r√©sultats: {', '.join(labels[:20])}... (et {len(data)-20} autres)"
            return f"{len(data)} r√©sultats: {', '.join(labels)}"

        if isinstance(data, dict):
            parts = []

            # Cas lookup_entity: {total, items}
            if 'items' in data and 'total' in data:
                items = data['items']
                labels = [item.get('label', '?') for item in items]
                parts.append(f"Trouv√© {data['total']} entit√©s: {', '.join(labels)}")

            # Cas get_neighbors: {investments, investors} ou {investments, companies}
            if 'companies' in data:
                companies = data['companies']
                labels = [c.get('label', '?') for c in companies]
                parts.append(f"{len(companies)} companies: {', '.join(labels)}")

            if 'investors' in data:
                investors = data['investors']
                labels = [i.get('label', '?') for i in investors]
                parts.append(f"{len(investors)} investors: {', '.join(labels)}")

            if 'investments' in data:
                investments = data['investments']
                # Extraire infos cl√©s des investissements
                inv_summaries = []
                for inv in investments[:10]:
                    amount = inv.get('raised_amount', 0)
                    year = inv.get('funded_year', '?')
                    currency = inv.get('raised_currency_code', '')
                    inv_summaries.append(f"{year}: {amount:,.0f} {currency}" if amount else f"{year}")
                parts.append(f"{len(investments)} investments: {', '.join(inv_summaries)}")
                if len(investments) > 10:
                    parts[-1] += f"... (et {len(investments)-10} autres)"

            if parts:
                return " | ".join(parts)

        # Fallback: repr√©sentation JSON tronqu√©e
        import json
        return json.dumps(data, ensure_ascii=False)[:1500]

    def run(self, query: str) -> str:
        """
        Ex√©cute une requ√™te d'investigation.

        Args:
            query: La question de l'utilisateur en langage naturel

        Returns:
            Le r√©sum√© des r√©sultats
        """
        print(f"\n{'='*60}")
        print(f"Nouvelle investigation: {query}")
        print(f"{'='*60}")

        # √âtat initial
        initial_state = {
            "query": query,
            "plan": "",
            "exploration_results": [],
            "explored_nodes": [],
            "candidate_nodes": [],
            "iteration": 0,
            "summary": "",
            "messages": [],
            "finished": False,
            "context_exceeded": False
        }

        # Ex√©cuter le graphe
        final_state = self.graph.invoke(initial_state)

        return final_state.get("summary", "Aucun r√©sultat trouv√©.")
